---
title: "Rapport de laboratoire 2"
subtitle: "MTH8408"
author:
  - name: Nicolas Jouglet
    email: nicolas.jouglet@polymtl.ca
    affiliation:
      - name: Polytechnique Montréal
format:
  pdf:
    keep-tex: false
    documentclass: article
    include-in-header:
      - text: |
            \usepackage{eulervm}
            \usepackage{xspace}
            \usepackage[francais]{babel}
    geometry:
      - margin=1in
    papersize: letter
    colorlinks: true
    urlcolor: blue
engine: julia
---

```{julia}
#| output: false
using Pkg
Pkg.activate("labo2_env")
Pkg.add("Printf")
using LinearAlgebra
using Printf
```

# Contexte

Dans ce laboratoire, on demande d'implémenter deux méthodes itératives pour résoudre
$$
  \min_x \ g^T x + \tfrac{1}{2} x^T H x
$$ {#eq-qp}
où $g \in \mathbb{R}^n$ et $H$ est une matrice $n \times n$ symétrique et définie positive.

# Question 1

En cours, nous avons vu la méthode de la plus forte pente avec recherche linéaire exacte pour résoudre ([-@eq-qp]).

Dans cette question, on demande d'implémenter et de tester cette méthode sur divers objectifs quadratiques convexes.

Votre implémentation doit avoir les caractéristiques suivantes :

1. un critère d'arrêt absolu et relatif sur le gradient de l'objectif ;
2. un critère d'arrêt portant sur le nombre d'itérations (le nombre maximum d'itérations devrait dépendre du nombre de variables $n$ du problème) ;
3. allouer un minimum en utilisant les opérations vectorisées (`.=`, `.+`, `.+=`, etc.) autant que possible ;
4. calculer *un seul* produit entre $H$ et un vecteur par itération ;
5. n'utiliser $H$ qu'à travers des produits avec un vecteur (ne pas accéder aux éléments de $H$ ou indexer dans $H$) ;
5. ne dépendre que de `LinearAlgebra`.
6. votre fonction principale doit être documentée---reportez-vous à [https://docs.julialang.org/en/v1/manual/documentation](https://docs.julialang.org/en/v1/manual/documentation) ;
7. votre fonction doit faire afficher les informations pertinentes à chaque itération sous forme de tableau comme vu en cours.

Tester votre implémentation sur les problèmes quadratiques de la section *Problèmes test* ci-dessous.

```{julia}

function steepest_qp(g, H, eps_a=1.0e-5, eps_r=1.0e-5)
"""
    steepest_qp(g, H[, eps_a=1.0e-5, eps_r=1.0e-5])

Résout un problème quadratique de la forme :

    min_x g'x + (1/2)x'Hx

en utilisant la méthode de descente du plus fort gradient (steepest descent).

# Arguments
- g::Vector{Float64}` : Le vecteur des coefficients linéaires.
- H::Matrix{Float64}` : La matrice de Hessienne (supposée symétrique définie positive).
- eps_a::Float64=1.0e-5` : Tolérance absolue pour le critère d'arrêt.
- eps_r::Float64=1.0e-5` : Tolérance relative pour le critère d'arrêt.

# Retourne
- x::Vector{Float64}` : Le vecteur minimiseur approché.
- k::Int` : Le nombre d’itérations effectuées.

# Détails
La direction de descente est donnée par `-gk` (le gradient courant), et le pas `alpha` est calculé par une règle analytique optimale pour les problèmes quadratiques : `alpha = (d'd)/(d'Hd)`.

La fonction s'arrête lorsque la norme du gradient est inférieure à `eps_a + eps_r * ||g₀||`, ou après `50n` itérations.
"""

    n = length(g)
    x = zeros(n)
    k = 0
    gk = g + H * x
    gnorm0=norm(gk)

    max_iter = 50n
    if max_iter ≤ 100
    print_every = 10
elseif max_iter ≤ 500
    print_every = 25
elseif max_iter ≤ 1000
    print_every = 50
else
    print_every = 100
end

    @printf("| %3s | %13s | %12s | %12s |\n", "k", "||gk||", "alpha", "f(x)")
    @printf("|%s|\n", "-"^53)

    while norm(gk) > eps_a + eps_r * gnorm0
        d = -gk

        alpha = dot(d, d) / dot(d, H*d)
        x .+= alpha * d
        Hx=H*x

        f = dot(g, x) + 0.5 * dot(x, Hx)

        if k % print_every == 0
            @printf("| %3d | %13.6e | %12.6e | %12.6e |\n", k, norm(gk), alpha, f)
        end

        gk = g + Hx
        k += 1

        if k > 10n
            println("Arrêt : trop d'itérations")
            break
        end
    end
        
    print("k = ", k, "; x = ",x )

    return  k,x
end

```


# Question 2

Dans cette question, on demande d'implémenter la méthode BFGS pour résoudre le problème quadratique convexe ([-@eq-qp]).

Votre implémentation doit avoir les mêmes caractéristiques qu'à la question 1.

Ici, on cherche notamment à valider le résultat disant que la méthode se termine en au plus $n$ itérations (en arithmétique exacte) et reconstruit $H$, c'est-à-dire que $B_k = H$ à la convergence.

Tester votre implémentation sur les problèmes quadratiques de la section *Problèmes test* ci-dessous.

```{julia}
function bfgs_qp(g, H, eps_a=1.0e-5, eps_r=1.0e-5)
   """
    bfgs_qp(g, H[, eps_a=1.0e-5, eps_r=1.0e-5])

Résout le problème quadratique `min_x g'x + 0.5 x'Hx` avec l'algorithme quasi-Newton BFGS.

# Arguments
- `g::Vector{Float64}` : Vecteur du terme linéaire.
- `H::Matrix{Float64}` : Matrice de Hessienne (supposée SPD).
- `eps_a::Float64=1.0e-5` : Tolérance absolue.
- `eps_r::Float64=1.0e-5` : Tolérance relative.

# Retourne
- `x` : Vecteur solution.
- `k` : Nombre d’itérations.
"""

    n = length(g)
    x = zeros(n)
    gk = g + H * x
    gnorm0= norm(gk)
    @printf("| %3s | %13s | %12s | %12s |\n", "k", "||gk||", "alpha", "f(x)")
    @printf("|%s|\n", "-"^53)
    k = 0
    Hk=I(n)

    while norm(gk) > eps_a + eps_r * gnorm0

        gk= g + H * x
        d = -Hk * gk
        
        alpha = -(gk' * d) / (d' * H * d)
  
        x .+= alpha * d
        Hx=H*x
        
        f = dot(g, x) + 0.5 * dot(x, Hx)

        gkp1=g+Hx
        yk = gkp1 - gk
        sk=alpha*d
        rho = 1.0 / dot(yk, sk)

        Hk=(I(n)-rho*sk*yk')*Hk*(I(n)-rho*yk*sk')+rho*sk*sk'
        
        @printf("| %3d | %13.6e | %12.6e | %12.6e |\n", k, norm(gk), alpha, f)

        k += 1
        if k > 2n
            println("Arrêt : trop d'itérations")
            break
        end
    end
    print(" k = ", k, "; x = ", x)
    return k, x
end

```

# Résultats numériques

## Problèmes test

Votre premier problème test sera généré aléatoirement avec $n = 10$.

```{julia}
n = 10
using Random

# Vecteur gradient aléatoire
g_rand = randn(n)

# Matrice symétrique définie positive
A = randn(n, n)
H_rand = A' * A + 1e-3 * I #pour que la matrice soit sym déf pos


# Affichage
println("g_rand = ", g_rand)
println("H_rand = ", H_rand)

steepest_qp(g_rand,H_rand)
bfgs_qp(g_rand, H_rand)

```

Utiliser ensuite les problèmes quadratiques convexes de la collection Maros et Meszaros.
Vous pouvez y accéder à l'aide de l'extrait de code suivant :
```{julia}
#| output: false
Pkg.add("QPSReader")  # collection + outils pour lire les problèmes

using QPSReader
using Logging
using SparseArrays

function get_gH(name, reg=0)
  mm_path = fetch_mm()  # chemin vers les problèmes sur votre disque
  qpdata = with_logger(Logging.NullLogger()) do
    readqps(joinpath(mm_path, name))
  end
  n = qpdata.nvar
  g = qpdata.c
  H = Symmetric(sparse(qpdata.qrows, qpdata.qcols, qpdata.qvals, n, n) + reg * I, :L)
  return g, H
end
```

Les noms des problèmes sont listés sur [https://bitbucket.org/optrove/maros-meszaros/src/master/](https://bitbucket.org/optrove/maros-meszaros/src/master/).

Leurs dimensions sont donnés dans le tableau sur la page [https://www.doc.ic.ac.uk/~im/00README.QP](https://www.doc.ic.ac.uk/%7Eim/00README.QP) (avec des noms qui ne correspondent pas tout à fait ; les noms corrects sont ceux de la page Bitbucket).

NB : ces problèmes ont des contraintes, mais dans ce laboratoire, on les ignore.

Choisissez 3 problèmes :

* un avec $n \approx 10$ ;
* un avec $n \approx 50$ ;
* un avec $n \approx 100$.

```{julia}
g9, H9 = get_gH("dualc1.SIF")

g79, H79 = get_gH("qshare2b.SIF", 1.0e-3)

g140, H140 = get_gH("qscagr7.SIF", 1.0e-3)

```



Attention :

* il se peut que $g = 0$---dans ce cas, changez $g$ en `ones(n)` ;
* il se peut que $H$ soit seulement semi-définie positive et pas définie positive---dans ce cas, ajoutez-lui un petit multiple de l'identité via, par exemple,

```julia
g, H = get_gH(name, 1.0e-3)
```

## Validation de la méthode de la plus forte pente

```{julia}

steepest_qp(g9,H9)
steepest_qp(g79,H79) 
steepest_qp(g140,H140)

```


## Validation de la méthode BFGS

```{julia}
bfgs_qp(g9,H9)
bfgs_qp(g79,H79) 
bfgs_qp(g140,H140)
```
