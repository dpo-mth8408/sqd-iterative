---
title: "Rapport de laboratoire - 2"
subtitle: "MTH8408"
author:
  - name: Atousa Pourfrad
    email: atousa.pourfard@polymtl.ca
    affiliation:
      - name: Polytechnique Montréal
format:
  pdf:
    keep-tex: false
    documentclass: article
    include-in-header:
      - text: |
            \usepackage{eulervm}
            \usepackage{xspace}
            \usepackage[francais]{babel}
    geometry:
      - margin=1in
    papersize: letter
    colorlinks: true
    urlcolor: blue
engine: julia
---

```{julia}
#| output: false
using Pkg
Pkg.activate("labo2_env")
using LinearAlgebra
using Printf  # J'ai ajouté pour afficher les résultats
```

# Contexte

Dans ce laboratoire, on demande d'implémenter deux méthodes itératives pour résoudre
$$
  \min_x \ g^T x + \tfrac{1}{2} x^T H x
$$ {#eq-qp}
où $g \in \mathbb{R}^n$ et $H$ est une matrice $n \times n$ symétrique et définie positive.

# Question 1

En cours, nous avons vu la méthode de la plus forte pente avec recherche linéaire exacte pour résoudre ([-@eq-qp]).

Dans cette question, on demande d'implémenter et de tester cette méthode sur divers objectifs quadratiques convexes.

Votre implémentation doit avoir les caractéristiques suivantes :

1. un critère d'arrêt absolu et relatif sur le gradient de l'objectif ;
2. un critère d'arrêt portant sur le nombre d'itérations (le nombre maximum d'itérations devrait dépendre du nombre de variables $n$ du problème) ;
2. toujours démarrer de l'approximation initiale $0$ ;
3. allouer un minimum en utilisant les opérations vectorisées (`.=`, `.+`, `.+=`, etc.) autant que possible ;
4. calculer *un seul* produit entre $H$ et un vecteur par itération ;
5. n'utiliser $H$ qu'à travers des produits avec un vecteur (ne pas accéder aux éléments de $H$ ou indexer dans $H$) ;
5. ne dépendre que de `LinearAlgebra`.
6. votre fonction principale doit être documentée---reportez-vous à [https://docs.julialang.org/en/v1/manual/documentation](https://docs.julialang.org/en/v1/manual/documentation) ;
7. votre fonction doit faire afficher les informations pertinentes à chaque itération sous forme de tableau comme vu en cours.

Tester votre implémentation sur les problèmes quadratiques de la section *Problèmes test* ci-dessous.

```{julia}
function steepest_qp(g::Vector, H::Matrix, n::Int, x::Vector=zeros(n), eps_a=1e-5, eps_r=1e-5)

    # H soit symétrique positive définie (SPD)
    function is_positive_definite(H::Matrix)
        isposdef(H) && issymmetric(H)
    end

    # Fonction objectif
    f(x) = dot(g, x) + 0.5 * dot(x, H * x)

    # Initialisation du gradient et des normes
    ∇fxk = g .+ H * x
    ∇fx0 = copy(∇fxk)
    norm∇fx0 = norm(∇fx0)
    norm∇fxk = norm(∇fxk)
    itermax = 100 * n

    @printf "%2s    %9s   %7s   %7s\n" "k" "f(x)" "‖∇fk‖" "‖x‖"

    for k in 0:itermax
        fx = f(x)  # Appel explicite de la fonction f(x)

        @printf "%2d    %9.2e   %7.1e   %7.1e\n" k fx norm∇fxk norm(x)

        # Critère d'arrêt (gradient petit)
        if norm∇fxk ≤ eps_a + (eps_r * norm∇fx0)
            break
        end

        # Critère de divergence (sécurité)
        if isnan(fx) || isinf(fx)
            println(" À k= ", k, " f(x) est divergent (NaN ou Inf). Arrêt de l'algorithme.")
            break
        end

        # Direction de descente (steepest descent)
        dk = -∇fxk
        t = 1.0  # Pas fixe pour la recherche exacte

        # Mise à jour du point
        x .+= t * dk
        ∇fxk .= g .+ H * x
        norm∇fxk = norm(∇fxk)
    end

    println("x optimal est: ", x)  # Affichage du point optimal
    return x
end
```

# Question 1: Résultats numériques
```{julia}
#Test de l'algorithme steepest_qp
n = 10
g = randn(n)
A = randn(n, n)
H = A' * A
x_optimal = steepest_qp(g, H, n);
```

## =========================================================

# Question 2

Dans cette question, on demande d'implémenter la méthode BFGS pour résoudre le problème quadratique convexe ([-@eq-qp]).

Votre implémentation doit avoir les mêmes caractéristiques qu'à la question 1.

Ici, on cherche notamment à valider le résultat disant que la méthode se termine en au plus $n$ itérations (en arithmétique exacte) et reconstruit $H$, c'est-à-dire que $B_k = H$ à la convergence.

Tester votre implémentation sur les problèmes quadratiques de la section *Problèmes test* ci-dessous.

```{julia}
function bfgs_qp(g::Vector, H::Matrix, n::Int, x::Vector=zeros(n), eps_a=1e-5, eps_r=1e-5)

    # Fonction objectif
    f(x) = dot(g, x) + 0.5 * dot(x, H * x)

    ∇fxk = g .+ H * x
    ∇fx0 = copy(∇fxk)
    norm∇fx0 = norm(∇fx0)
    norm∇fxk = norm(∇fxk)
    itermax = n
    k = 0
    B = Matrix{Float64}(I, n, n)  # Approximation initiale de l'inverse du Hessien

    @printf "%2s    %9s   %7s   %7s\n" "k" "f(x)" "‖∇fk‖" "‖x‖"

    for k in 0:itermax
        @printf "%2d    %9.2e   %7.1e   %7.1e\n" k f(x) norm∇fxk norm(x)

        # Critère d'arrêt
        if norm∇fxk <= eps_a + (eps_r * norm∇fx0)
            break
        end

        dk = -B * ∇fxk  # Direction de descente
        t = 1.0 # Recherche exacte (fixe)
        xk = x + t * dk  # Mise à jour de x
        ∇fxk1 = g .+ H * xk  # Nouveau gradient

        # Calcul de s et y pour la mise à jour BFGS
        s = xk - x
        y = ∇fxk1 - ∇fxk
        ρ = 1.0 / dot(y, s)

        # Mise à jour BFGS
        B = (I - ρ * s * y') * B * (I - ρ * y * s') + ρ * (s * s')

        # Mise à jour des variables
        x = xk
        ∇fxk = ∇fxk1
        norm∇fxk = norm(∇fxk)
    end
    println("x optimal est: ", x)  # Affichage du point optimal
    return x
end
```

# Question 2: Résultats numériques

```{julia}
# Test numérique
n = 10
g = randn(n)  # Gradient aléatoire
B = Matrix{Float64}(I(n))  # Approximation initiale de H⁻¹ (identité)
x0 = zeros(n)  # Point de départ pour x
x_optimal = bfgs_qp(g, B, n, x0);

```
### Note: Le resultat ne passe jamais plus d'une itération.

## =========================================================

# Résultats numériques

## Problèmes test

Votre premier problème test sera généré aléatoirement avec $n = 10$.

```{julia}
n = 10
# g_rand = ...
# H_rand = ...
```

Utiliser ensuite les problèmes quadratiques convexes de la collection Maros et Meszaros.
Vous pouvez y accéder à l'aide de l'extrait de code suivant :
```{julia}
#| output: false
Pkg.add("QPSReader")  # collection + outils pour lire les problèmes

using QPSReader
using Logging
using SparseArrays

function get_gH(name, reg=0)
    mm_path = fetch_mm()  # chemin vers les problèmes sur votre disque
    qpdata = with_logger(Logging.NullLogger()) do
        readqps(joinpath(mm_path, name))
    end
    n = qpdata.nvar
    g = qpdata.c
    H = Symmetric(sparse(qpdata.qrows, qpdata.qcols, qpdata.qvals, n, n) + reg * I, :L)
    return g, H
end
```

Les noms des problèmes sont listés sur [https://bitbucket.org/optrove/maros-meszaros/src/master/](https://bitbucket.org/optrove/maros-meszaros/src/master/).

Leurs dimensions sont donnés dans le tableau sur la page [https://www.doc.ic.ac.uk/~im/00README.QP](https://www.doc.ic.ac.uk/%7Eim/00README.QP) (avec des noms qui ne correspondent pas tout à fait ; les noms corrects sont ceux de la page Bitbucket).

NB : ces problèmes ont des contraintes, mais dans ce laboratoire, on les ignore.

Choisissez 3 problèmes :

* un avec $n \approx 10$ ;
* un avec $n \approx 50$ ;
* un avec $n \approx 100$.

```{julia}
# Énoncé du lab
# g100, H100 = get_gH("CVXQP1_S.SIF") 
```

```{julia}
g10, H10 = get_gH("GENHS28.SIF") #N=number of variables=10
```

```{julia}
g79, H79 = get_gH("QSHARE2B.SIF") #N=number of variables=79
```

```{julia}
g100, H100 = get_gH("CVXQP3_S.SIF") #N=number of variables=100
```

Attention :

* il se peut que $g = 0$---dans ce cas, changez $g$ en `ones(n)` ;
* il se peut que $H$ soit seulement semi-définie positive et pas définie positive---dans ce cas, ajoutez-lui un petit multiple de l'identité via, par exemple,

```{julia}
g10, H10 = get_gH("GENHS28.SIF", 1.0e-3)
```

```{julia}
g79, H79 = get_gH("QSHARE2B.SIF", 1.0e-3)
```

```{julia}
g100, H100 = get_gH("CVXQP3_S.SIF", 1.0e-3)
```

## Validation de la méthode de la plus forte pente

```{julia}
#Test de l'algorithme steepest_qp
n = 10
g = g10
A = randn(n, n)
H = H10
x_optimal = steepest_qp(g, H, n);
```

```{julia}
#Test de l'algorithme steepest_qp
n = 79
g = g79
A = randn(n, n)
H = H79
x_optimal = steepest_qp(g, H, n); 
```

```{julia}
#Test de l'algorithme steepest_qp
n = 100
g = g100
A = randn(n, n)
H = H100
x_optimal = steepest_qp(g, H, n); 
```

## Validation de la méthode BFGS

```{julia}
# Test numérique
n = 10
g = g10
B = Matrix{Float64}(I(n))  # Approximation initiale de H⁻¹ (identité)
x0 = zeros(n)  # Point de départ pour x
x_optimal = bfgs_qp(g, B, n, x0);

```

```{julia}
# Test numérique
n = 79
g = g79
B = Matrix{Float64}(I(n))  # Approximation initiale de H⁻¹ (identité)
x0 = zeros(n)  # Point de départ pour x
x_optimal = bfgs_qp(g, B, n, x0);

```

```{julia}
# Test numérique
n = 100
g = g100
B = Matrix{Float64}(I(n))  # Approximation initiale de H⁻¹ (identité)
x0 = zeros(n)  # Point de départ pour x
x_optimal = bfgs_qp(g, B, n, x0);

```
